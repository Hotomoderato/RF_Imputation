{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "class RandomForestImputer:\n",
    "    def __init__(self, featuresCols, labelCol, predictionCol=\"prediction\"):\n",
    "        self.featuresCols = featuresCols\n",
    "        self.labelCol = labelCol\n",
    "        self.predictionCol = predictionCol\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        imputer = Imputer(inputCols=self.featuresCols, outputCols=[f\"{c}_imputed\" for c in self.featuresCols])\n",
    "        assembler = VectorAssembler(inputCols=[f\"{c}_imputed\" for c in self.featuresCols], outputCol=\"features\")\n",
    "        classifier = RandomForestClassifier(labelCol=self.labelCol, featuresCol=\"features\", predictionCol=self.predictionCol)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=[imputer, assembler, classifier])\n",
    "        self.model = self.pipeline.fit(dataset)\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        return self.model.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = RandomForestImputer(featuresCols=[\"col1\", \"col2\", \"col3\"], labelCol=\"label\")\n",
    "imputer_model = imputer.fit(dataset)\n",
    "imputed_dataset = imputer_model.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "class RandomForestImputer:\n",
    "    def __init__(self, featuresCols, labelCol, predictionCol=\"prediction\"):\n",
    "        self.featuresCols = featuresCols\n",
    "        self.labelCol = labelCol\n",
    "        self.predictionCol = predictionCol\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        stages = []\n",
    "        \n",
    "        # One-hot encode categorical features\n",
    "        categorical_features = []\n",
    "        for col in self.featuresCols:\n",
    "            if dataset.schema[col].dataType == \"string\":\n",
    "                categorical_features.append(col)\n",
    "                indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\")\n",
    "                encoder = OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\")\n",
    "                stages.extend([indexer, encoder])\n",
    "        \n",
    "        # Impute missing values\n",
    "        imputer_cols = [col for col in self.featuresCols if col not in categorical_features]\n",
    "        imputer_cols = [f\"{c}_vec\" if c in categorical_features else c for c in imputer_cols]\n",
    "        imputer = Imputer(inputCols=imputer_cols, outputCols=[f\"{c}_imputed\" for c in imputer_cols])\n",
    "        stages.append(imputer)\n",
    "        \n",
    "        # Assemble features into a single vector\n",
    "        assembler = VectorAssembler(inputCols=[f\"{c}_imputed\" if c not in categorical_features else f\"{c}_vec\" for c in self.featuresCols], outputCol=\"features\")\n",
    "        stages.append(assembler)\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        classifier = RandomForestClassifier(labelCol=self.labelCol, featuresCol=\"features\", predictionCol=self.predictionCol)\n",
    "        stages.append(classifier)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.model = self.pipeline.fit(dataset)\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        return self.model.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "class RandomForestImputer:\n",
    "    def __init__(self, featuresCols, labelCol, predictionCol=\"prediction\"):\n",
    "        self.featuresCols = featuresCols\n",
    "        self.labelCol = labelCol\n",
    "        self.predictionCol = predictionCol\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        stages = []\n",
    "        \n",
    "        # One-hot encode categorical features\n",
    "        categorical_features = []\n",
    "        for col in self.featuresCols:\n",
    "            if dataset.schema[col].dataType == \"string\":\n",
    "                categorical_features.append(col)\n",
    "                indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\")\n",
    "                encoder = OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\")\n",
    "                stages.extend([indexer, encoder])\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        assembler = VectorAssembler(inputCols=[f\"{c}_vec\" if c in categorical_features else c for c in self.featuresCols], outputCol=\"features\")\n",
    "        stages.append(assembler)\n",
    "        regressor = RandomForestClassifier(labelCol=self.labelCol, featuresCol=\"features\", predictionCol=self.predictionCol)\n",
    "        stages.append(regressor)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.model = self.pipeline.fit(dataset)\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        dataset = self.model.transform(dataset)\n",
    "        for col in self.featuresCols:\n",
    "            if dataset.schema[col].nullable:\n",
    "                dataset = dataset.na.fill({col: dataset.agg({self.predictionCol: \"mean\"}).first()[0]})\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "class RandomForestImputer:\n",
    "    def __init__(self, featuresCols, labelCol, predictionCol=\"prediction\"):\n",
    "        self.featuresCols = featuresCols\n",
    "        self.labelCol = labelCol\n",
    "        self.predictionCol = predictionCol\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        stages = []\n",
    "        \n",
    "        # One-hot encode categorical features\n",
    "        categorical_features = []\n",
    "        for col in self.featuresCols:\n",
    "            if dataset.schema[col].dataType == \"string\":\n",
    "                categorical_features.append(col)\n",
    "                indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\")\n",
    "                encoder = OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\")\n",
    "                stages.extend([indexer, encoder])\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        assembler = VectorAssembler(inputCols=[f\"{c}_vec\" if c in categorical_features else c for c in self.featuresCols], outputCol=\"features\")\n",
    "        stages.append(assembler)\n",
    "        regressor = RandomForestRegressor(labelCol=self.labelCol, featuresCol=\"features\", predictionCol=self.predictionCol)\n",
    "        stages.append(regressor)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.model = self.pipeline.fit(dataset)\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        dataset = self.model.transform(dataset)\n",
    "        for col in self.featuresCols:\n",
    "            if dataset.schema[col].nullable:\n",
    "                if dataset.schema[col].dataType == \"double\":\n",
    "                    mean = dataset.agg({col: \"mean\"}).first()[0]\n",
    "                    dataset = dataset.na.fill({col: mean})\n",
    "                elif dataset.schema[col].dataType == \"string\":\n",
    "                    most_frequent = dataset.groupBy(col).count().sort(\"count\", ascending=False).first()[0]\n",
    "                    dataset = dataset.na.fill({col: most_frequent})\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import isnan, when, count, avg\n",
    "\n",
    "class RandomForestImputer:\n",
    "    def __init__(self, categorical_cols, continuous_cols, label_col):\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self.string_indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\")\n",
    "                                for c in self.categorical_cols]\n",
    "        self.encoder = OneHotEncoderEstimator(inputCols=[f\"{c}_index\" for c in self.categorical_cols],\n",
    "                                              outputCols=[f\"{c}_encoded\" for c in self.categorical_cols])\n",
    "        self.assembler = VectorAssembler(inputCols=[f\"{c}_encoded\" for c in self.categorical_cols] + self.continuous_cols,\n",
    "                                         outputCol=\"features\")\n",
    "        self.regressor = RandomForestRegressor(featuresCol=\"features\", labelCol=self.label_col)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=self.string_indexers + [self.encoder, self.assembler, self.regressor])\n",
    "        self.model = self.pipeline.fit(dataset)\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        self.fitted_dataset = self.model.transform(dataset)\n",
    "\n",
    "        def fill_missing_value(row):\n",
    "            for c in self.categorical_cols + self.continuous_cols:\n",
    "                if row[c] is None or row[c] == \"\":\n",
    "                    row_features = row[[\"features\"] + self.categorical_cols + self.continuous_cols]\n",
    "                    filled_value = self.model.transform(row_features.fillna(\"\"))\n",
    "                    return row.withColumn(c, filled_value.select(self.label_col).first()[0])\n",
    "            return row\n",
    "\n",
    "        self.filled_dataset = self.fitted_dataset.rdd.map(fill_missing_value).toDF()\n",
    "        return self.filled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputeMissingValuesWithRandomForest:\n",
    "    def __init__(self, categorical_features, numerical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.missing_values_dict = {}\n",
    "    \n",
    "    def fit(self, dataframe):\n",
    "        # 对连续型特征进行编码\n",
    "        for feature in self.numerical_features:\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            dataframe[feature] = imputer.fit_transform(dataframe[[feature]])\n",
    "        # 对离散型特征进行编码\n",
    "        for feature in self.categorical_features:\n",
    "            dataframe[feature] = dataframe[feature].astype('category')\n",
    "            dataframe[feature].fillna(dataframe[feature].mode()[0], inplace=True)\n",
    "        \n",
    "        # 对缺失值进行记录\n",
    "        for col in dataframe.columns:\n",
    "            missing_values_index = dataframe[col].index[dataframe[col].apply(np.isnan)]\n",
    "            for i in missing_values_index:\n",
    "                if i not in self.missing_values_dict:\n",
    "                    self.missing_values_dict[i] = {}\n",
    "                self.missing_values_dict[i][col] = np.nan\n",
    "        \n",
    "        # 进行缺失值填充\n",
    "        filled_dataframe = dataframe.fillna(-999)\n",
    "        # 建立随机森林模型并训练\n",
    "        X = filled_dataframe.drop(columns='target')\n",
    "        y = filled_dataframe['target']\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # 预测缺失值\n",
    "        for i, row in dataframe.iterrows():\n",
    "            if i not in self.missing_values_dict:\n",
    "                continue\n",
    "            for col in self.missing_values_dict[i]:\n",
    "                dataframe.at[i, col] = model.predict(row.drop(col))[0]\n",
    "        \n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
